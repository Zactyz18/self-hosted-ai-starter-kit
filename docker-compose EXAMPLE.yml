version: "3.9"

############
# Networks #
############
networks:
  demo:

###########
# Volumes #
###########
volumes:
  dbdata:
  n8n_data:
  ollama_storage:
  qdrant_storage:

############
# Services #
############
services:
  # ---------- DATABASE ---------- #
  postgres:
    image: postgres:16
    hostname: postgres          # <-- keep name stable for env vars
    env_file: .env
    networks: [demo]
    restart: unless-stopped
    volumes:
      - dbdata:/var/lib/postgresql/data
      - ./db/init_simple.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -h localhost -U $$POSTGRES_USER -d $$POSTGRES_DB"]
      interval: 5s
      timeout: 5s
      retries: 10

  # ---------- MAIN N8N UI/API ---------- #
  n8n:
    build:
      context: ./n8n
      dockerfile: Dockerfile
    hostname: n8n
    env_file: .env
    networks: [demo]
    restart: unless-stopped
    environment:
      - DB_TYPE=postgresdb
      - DB_POSTGRESDB_HOST=postgres
      - DB_POSTGRESDB_DATABASE=${POSTGRES_DB:-n8n}
      - DB_POSTGRESDB_USER=${POSTGRES_USER:-pdfai}
      - DB_POSTGRESDB_PASSWORD=${POSTGRES_PASSWORD:-pdfai}
      - N8N_ENCRYPTION_KEY=${N8N_ENCRYPTION_KEY:-o0k8GTCG3ojM8w/Gb10vQ1wsM1Nb9YM1}
      
      # URL configuration using environment variables
      - N8N_EDITOR_BASE_URL=${N8N_EDITOR_BASE_URL:-http://localhost/n8n/}
      - N8N_PUBLIC_URL=${N8N_PUBLIC_URL:-http://localhost/n8n/}
      - N8N_PATH=/n8n/
      - WEBHOOK_URL=${WEBHOOK_URL:-http://localhost/n8n/webhook/}
      
      # Authentication
      - N8N_USER=${N8N_USER:-admin}
      - N8N_PASS=${N8N_PASS:-Password1}
      - N8N_USER_MANAGEMENT_JWT_SECRET=${N8N_USER_MANAGEMENT_JWT_SECRET:-23487thv82h2h4vh23985hdg}
      - N8N_USER_MANAGEMENT_DISABLED=false
      
      # API and CORS
      - N8N_PUBLIC_API_DISABLED=false
      - N8N_API_SECURITY_AUDIT_LOGS_DISABLED=false
      - N8N_CORS_ALLOWED_ORIGINS=${N8N_CORS_ALLOWED_ORIGINS:-http://localhost}
      - N8N_CORS_ALLOW_CREDENTIALS=true
      
      # File handling
      - N8N_DEFAULT_BINARY_DATA_MODE=filesystem
      - N8N_IMPORT_EXPORT_FOLDER=/home/node/.n8n/import
      - N8N_IMPORT_EXPORT_SKIP_PROMPT=1
      
      # Ollama integration
      - OLLAMA_HOST=ollama:11434
    volumes:
      - n8n_data:/home/node/.n8n
      - ./n8n/import:/home/node/.n8n/import
    depends_on:
      postgres:
        condition: service_healthy

  # ---------- YOUR PYTHON WORKER ---------- #
  worker:
    build: ./worker
    networks: [demo]
    env_file: .env
    environment:
      - S3_BUCKET_NAME=${S3_BUCKET_NAME}
    volumes:
      - ./worker/classifier.py:/data/classifier.py
      - ./worker/splitter.py:/data/splitter.py
    ports: ["9000:9000"]

  # ---------- FRONTEND (Next.js) ---------- #
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        - NEXT_PUBLIC_N8N_URL=${NEXT_PUBLIC_N8N_URL}
    networks: [demo]
    environment:
      - NODE_ENV=production
      - NEXT_PUBLIC_N8N_URL=${NEXT_PUBLIC_N8N_URL}
    restart: unless-stopped
    depends_on: [n8n]

  # ---------- PGADMIN ---------- #
  pgadmin:
    image: dpage/pgadmin4:latest
    networks: [demo]
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@admin.com
      PGADMIN_DEFAULT_PASSWORD: admin
      PGADMIN_CONFIG_SERVER_MODE: "False"
      PGADMIN_CONFIG_MASTER_PASSWORD_REQUIRED: "False"
    ports: ["5050:80"]
    volumes:
      - ./pgadmin/servers.json:/pgadmin4/servers.json
    depends_on:
      - postgres

  # ---------- NGINX REVERSE-PROXY ---------- #
  nginx:
    image: nginx:latest
    networks: [demo]
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"  # Enable HTTPS for development
    volumes:
      # Use local-proxy.conf for local testing
      - ./local-proxy.conf:/etc/nginx/conf.d/default.conf:ro
      # Mount SSL certificates for HTTPS development
      - ./ssl:/etc/nginx/ssl:ro
      # Use ec2-loadbalancer-nginx.conf for EC2 deployment
      # - ./ec2-proxy.conf:/etc/nginx/conf.d/default.conf:ro
    depends_on:
      - frontend
      - n8n

  # ---------- QDRANT VECTOR DB ---------- #
  qdrant:
    image: qdrant/qdrant
    hostname: qdrant
    networks: [demo]
    restart: unless-stopped
    ports: ["6333:6333"]
    volumes:
      - qdrant_storage:/qdrant/storage

  # ---------- OLLAMA (CPU by default) ---------- #
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    networks: [demo]
    # ---------- GPU ----------
    profiles: ["gpu-nvidia"]      # docker compose --profile gpu-nvidia up …
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    devices:                      # <-- Same indentation as 'deploy:', not nested under it
      - /dev/nvidia0:/dev/nvidia0
      - /dev/nvidiactl:/dev/nvidiactl
      - /dev/nvidia-uvm:/dev/nvidia-uvm
      - /dev/nvidia-modeset:/dev/nvidia-modeset  
    environment:
      NVIDIA_VISIBLE_DEVICES: all # expose all GPUs
      NVIDIA_DRIVER_CAPABILITIES: all # expose compute, utility, video, etc.
    # --------------------------
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama_storage:/root/.ollama       # cache models here

  # OPTIONAL: NVIDIA GPU build – start with `--profile gpu-nvidia`
  ollama-pull-llava:
    image: ollama/ollama:latest
    profiles: ["gpu-nvidia"]
    networks: [demo]
    depends_on:
      ollama:
        condition: service_started
    environment:
      OLLAMA_HOST: http://ollama:11434        # tell CLI where the server is
    entrypoint: /bin/sh -c "
        echo '⏳  Waiting for Ollama…';
        until ollama list >/dev/null 2>&1; do
          sleep 2;
        done;
        echo '⬇️  Pulling qwen2.5vl:7b…';
        ollama pull qwen2.5vl:7b"
    volumes:
      - ollama_storage:/root/.ollama
    restart: 'no'

